{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725d6fe2-0e57-4d60-b51d-8d4483c3a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#rugpt3large_based_on_gpt2\n",
    "model_rugpt3large = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\n",
    "word_embeddings_rugpt3large = model_rugpt3large.transformer.wte.weight  # Word Token Embeddings \n",
    "position_embeddings_rugpt3large = model_rugpt3large.transformer.wpe.weight  # Word Position Embeddings \n",
    "\n",
    "tokenizer_rugpt3large = GPT2Tokenizer.from_pretrained(\"sberbank-ai/rugpt3large_based_on_gpt2\")\n",
    "\n",
    "# MGPT\n",
    "model_mgpt = GPT2LMHeadModel.from_pretrained('sberbank-ai/mGPT')\n",
    "word_embeddings_mgpt = model_mgpt.transformer.wte.weight  # Word Token Embeddings \n",
    "position_embedding_mgpt = model_mgpt.transformer.wpe.weight  # Word Position Embeddings \n",
    "\n",
    "tokenizer_mgpt = GPT2Tokenizer.from_pretrained(\"sberbank-ai/mGPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f5a82-9fe4-4e07-94ce-2c31b0128540",
   "metadata": {},
   "source": [
    "# Far Away From Center with NGT ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "913ae0d8-64f5-4d65-bb8c-d046fcd5b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "word_embeddings_centroid_mgpt = torch.mean(word_embeddings_mgpt, dim=0)\n",
    "word_embeddings_sum_mgpt = torch.sum(word_embeddings_mgpt, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc84300-33d2-454d-aa5d-0046f71cdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngtpy\n",
    "\n",
    "# dim = 1536\n",
    "dim = 2048\n",
    "\n",
    "# ngtpy.create(b\"mgpt_Word_Embs\", dim)\n",
    "# index = ngtpy.Index(b\"mgpt_Word_Embs\")\n",
    "# index.batch_insert(word_embeddings_mgpt.detach().numpy())\n",
    "# index.save()\n",
    "index = ngtpy.Index(b\"mgpt_Word_Embs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6607884-6150-421c-af3d-cc3df7039b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = word_embeddings[468].detach().numpy()\n",
    "query = word_embeddings_centroid_mgpt.detach().numpy()\n",
    "results = index.search(query, 100000, epsilon=100.0, edge_size=30000)\n",
    "# results = index.linear_search(query, 100000)\n",
    "anomaly_tokens = []\n",
    "for i, (id, distance) in enumerate(results) :\n",
    "    if i > 99000:\n",
    "        # print(str(i) + \": \" + str(id) + \", \" + str(distance))\n",
    "        # print(tokenizer.decode([id]))\n",
    "        anomaly_tokens.append(tokenizer_mgpt.decode([id]))\n",
    "        # object = index.get_object(id)\n",
    "with open(\"anomaly_tokens_mgpt.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(anomaly_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4199186f-4173-44d5-b089-5271fe9ff283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_decoded = []\n",
    "# for i in range(50257):\n",
    "#     vocab_decoded.append(tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5148e5f-1b2e-4546-bc1a-a1226b6f5070",
   "metadata": {},
   "source": [
    "# K-MEANS Grouping method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73673a10-f7ac-4326-97ee-1f1566b4c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for n_clusters in [50,200,500,2000]:\n",
    "    # making k-means grouping\n",
    "    # X = word_embeddings_mgpt.detach()\n",
    "    X = word_embeddings_rugpt3large.detach()\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    \n",
    "    # grouping in dict\n",
    "    embs_groups = {}\n",
    "    for idx, label in enumerate(kmeans.labels_):\n",
    "        # tokens_list = embs_groups.get(label, []) + [tokenizer_mgpt.decode([idx])]\n",
    "        tokens_list = embs_groups.get(label, []) + [tokenizer_rugpt3large.decode([idx])]\n",
    "        embs_groups[label] = tokens_list\n",
    "    \n",
    "    # dumping in txt files\n",
    "    os.makedirs(f'ruGPT3-large_{str(n_clusters)}clusters', exist_ok = True)\n",
    "    for i in range(len(embs_groups)):\n",
    "        cluster_num = str(i).rjust(4, '0')\n",
    "        with open(f'ruGPT3-large_{str(n_clusters)}clusters/cluster_№{cluster_num}.txt', 'w') as f:\n",
    "            f.write(\"\\n\".join(embs_groups[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e48720-a8ee-42f8-be77-bfd5c932a6c7",
   "metadata": {},
   "source": [
    "# Testing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53cd5659-30b6-423d-af63-99eabe2cb882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u061c'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \" \" #Medium Mathematical Space (MMSP)\n",
    "test_str = \" \" #No-Break Space (NBSP)\n",
    "test_str = \" \" #Mac Space\n",
    "test_str = \"؜\" # ARABIC LETTER MARK\n",
    "\n",
    "tokenizer_mgpt.decode(tokenizer_mgpt.encode(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96e79d88-89ed-4517-a275-8d307dca0724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "уру\n",
      " с\n",
      "ских\n"
     ]
    }
   ],
   "source": [
    "# messing with generation placing spaces cluster1753\n",
    "test_str = \"ури нотерапия\"\n",
    "test_str = \"уре тра\"\n",
    "test_str = \"куры парня\"\n",
    "test_str = \"шуры парня\"\n",
    "test_str = \"уру сских\"\n",
    "# test_str = \"ура нтоварищи\"\n",
    "\n",
    "for token in tokenizer_mgpt.encode(test_str):\n",
    "    print(tokenizer_mgpt.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bbca004e-3d83-4d99-ad63-fa896faff73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вид\n",
      " экономики\n",
      " �\n",
      "�\n"
     ]
    }
   ],
   "source": [
    "# messing with generation placing spaces cluster1767\n",
    "test_str = \"вид экономики ؜\"\n",
    "\n",
    "for token in tokenizer_mgpt.encode(test_str):\n",
    "    print(tokenizer_mgpt.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0a91098e-c28e-4a55-b903-bfb5a72e486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "mas\n",
      "k\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "# messing with generation placing spaces cluster1801\n",
    "test_str = \"<mask>\"\n",
    "\n",
    "for token in tokenizer_mgpt.encode(test_str):\n",
    "    print(tokenizer_mgpt.decode([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0b9a3-13b4-48f0-a17f-f96954690116",
   "metadata": {},
   "source": [
    "## ruGPT3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "558ad90b-08a4-4555-a6d6-33ba9c078d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "m\n",
      "ask\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "# messing with generation placing spaces cluster1801\n",
    "test_str = \"<mask>\"\n",
    "\n",
    "for token in tokenizer_rugpt3large.encode(test_str):\n",
    "    print(tokenizer_rugpt3large.decode([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98d397-ac1b-4926-98e3-d75c243ee79a",
   "metadata": {},
   "source": [
    "# exploring inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c47aa81-bc7f-4f6a-8eb7-e2e09846fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mgpt.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "08b9b5f2-d353-4680-8a26-67657621d5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300000])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "test_str = \"уру сских\"\n",
    "ids = tokenizer_mgpt.encode(test_str, return_tensors=\"pt\")\n",
    "model_mgpt(ids)[0].view(batch_size,-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa1f8a-ffe1-453c-9b1e-d8197670257e",
   "metadata": {},
   "source": [
    "# what if we just select closest embedding to sum of prompt embs?\n",
    "we get embedding of words in it lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "50f89798-b814-42da-8902-cf6dd37a39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Р\n",
      "оссий\n",
      "ская\n",
      " федера\n",
      "ция\n",
      " чув\n",
      "ствует\n",
      " себя\n",
      " замеч\n",
      "ательно\n",
      " в\n",
      " этом\n",
      " эконом\n",
      "ическом\n",
      " цик\n",
      "ле\n"
     ]
    }
   ],
   "source": [
    "test_str = \"Российская федерация чувствует себя замечательно в этом экономическом цикле\"\n",
    "\n",
    "result_emb = torch.zeros((1,2048))\n",
    "for token in tokenizer_mgpt.encode(test_str):\n",
    "    result_emb += word_embeddings_mgpt[token]\n",
    "    print(tokenizer_mgpt.decode([token]))\n",
    "# result_emb /= len(tokenizer_mgpt.encode(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b9d308fc-cfa8-4fac-bc75-e96dca8138e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " в\n",
      " себя\n",
      " и\n",
      " на\n",
      " с\n",
      "ская\n",
      " не\n",
      "ле\n",
      " к\n",
      " от\n",
      " федера\n",
      " В\n",
      " этом\n",
      " по\n",
      " т\n"
     ]
    }
   ],
   "source": [
    "results = index.search(result_emb.detach().numpy(), 15)\n",
    "for i, (id, distance) in enumerate(results) :\n",
    "    print(tokenizer_mgpt.decode([id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda5374-70ec-4741-8437-907e9a4da972",
   "metadata": {},
   "source": [
    "# Misha's idea with circle group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a16e664b-fd9a-4a1a-81a4-5de43fca3e19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 955/2000 [14:19<07:54,  2.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1548/2000 [20:17<04:54,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n",
      "tensor([], grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [26:04<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "emb_clusters = {}\n",
    "# distance_info_clusters = {}\n",
    "word_clusters = sorted(glob('mGPT_2000clusters/*'))\n",
    "for cluster_fn in tqdm(word_clusters, total=len(word_clusters)):\n",
    "    # cluster_emb = torch.zeros((1,2048))\n",
    "    with open(cluster_fn, 'r') as f:    \n",
    "        tokens = f.read().split('\\n')\n",
    "    euclid_dist_embs = [0] * len(tokens)\n",
    "    for token_utf8_idx in range(len(tokens)):\n",
    "        for token_utf8_idx_next in range(token_utf8_idx+1, len(tokens)):\n",
    "            # print(token_utf8)\n",
    "            curr_token_emb = None\n",
    "            next_token_emb = None\n",
    "            token_curr = tokenizer_mgpt.encode(tokens[token_utf8_idx])\n",
    "            token_next = tokenizer_mgpt.encode(tokens[token_utf8_idx_next])\n",
    "            # print(token_curr, token_next)\n",
    "            if len(token_curr) > 1 or len(token_next) > 1:\n",
    "                continue\n",
    "            try:\n",
    "                curr_token_emb = word_embeddings_mgpt[token_curr]\n",
    "                next_token_emb = word_embeddings_mgpt[token_next]\n",
    "                emb_clusters[cluster_fn] = curr_token_emb\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            # euclidean distance https://stackoverflow.com/questions/68220457/to-calculate-euclidean-distance-between-vectors-in-a-torch-tensor-with-multiple\n",
    "            try:\n",
    "                dist = (curr_token_emb - next_token_emb).pow(2).sum(1).sqrt()\n",
    "            except:\n",
    "                print(curr_token_emb.shape, next_token_emb.shape)\n",
    "            try:\n",
    "                euclid_dist_embs[token_utf8_idx] += float(dist)\n",
    "                euclid_dist_embs[token_utf8_idx_next] += float(dist)\n",
    "            except:\n",
    "                print(dist, token)\n",
    "        # distance_info_clusters[cluster_fn] = [token_curr, tokens, euclid_dist_embs]\n",
    "        \n",
    "    euclid_dist_embs = [i for i in euclid_dist_embs if i != 0.0]\n",
    "#     print('\\nв группе', tokens, '\\n самый далекий ото всех', tokens[euclid_dist_embs.index(max(euclid_dist_embs))], \n",
    "#           '\\nк самому близкому у него отношение', max(euclid_dist_embs)/min(euclid_dist_embs), \n",
    "#           '\\n самый близкий ко всем', tokens[euclid_dist_embs.index(min(euclid_dist_embs))], )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0e68041c-ea79-46ab-85e0-f377894cd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('distance_info_clusters.pkl', 'wb') as f:\n",
    "    pickle.dump(distance_info_clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d7ebb0e8-ed95-4dd2-b9fe-0a8df31af356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distance_info_clusters) # distance_info_clusters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5df8a937-52d7-475f-967a-2146d2085770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_embs = []\n",
    "for info_pack in distance_info_clusters.values():\n",
    "    token_curr = tokenizer_mgpt.encode(info_pack[1][info_pack[2].index(max(info_pack[2]))])\n",
    "    curr_token_emb = word_embeddings_mgpt[token_curr]\n",
    "    token_embs.append(curr_token_emb)\n",
    "    # print(info_pack[1][info_pack[2].index(max(info_pack[2]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0bff2c3f-e8e1-4a57-a7aa-bc8018c96f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(token_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209e1f9-8fad-4c94-92a7-7283c3906991",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Carex\"\n",
    "\n",
    "result_emb = torch.zeros((1,2048))\n",
    "for token in tokenizer_mgpt.encode(test_str):\n",
    "    result_emb += word_embeddings_mgpt[token]\n",
    "    print(tokenizer_mgpt.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "94c0397a-5e78-43b3-b529-50d62ccb5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2048\n",
    "\n",
    "ngtpy.create(b\"mgpt_Word_Embs_2kClusters_Misha_RingIdea\", dim)\n",
    "index = ngtpy.Index(b\"mgpt_Word_Embs_2kClusters_Misha_RingIdea\")\n",
    "index.batch_insert(torch.cat(token_embs, 0).detach().numpy())\n",
    "index.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d99f4-5ca3-4193-bf0e-66d7694a8e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
